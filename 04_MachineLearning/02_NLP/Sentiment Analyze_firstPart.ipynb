{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python and Google’s Natural Language API"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Install the libraries\n",
    "We are going to use tweepy to gather the tweet data. We will use nltk to help us clean the tweets. Google Natural Language API will do the sentiment analysis. python-telegram-bot will send the result through Telegram chat.\n",
    "\n",
    "```\n",
    "pip3 install tweepy nltk google-cloud-language python-telegram-bot\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Get Twitter API Keys\n",
    "To be able to gather the tweets from Twitter, we need to create a developer account to get the Twitter API Keys first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Enable Google Natural Language API\n",
    "We need to enable the Google Natural Language API first if we want to use the service.\n",
    "\n",
    "Go to Google Developers Console and create a new project (or select the one you have).\n",
    "\n",
    "In the project dashboard, click “ENABLE APIS AND SERVICES”, and search for Cloud Natural Language API.\n",
    "\n",
    "### 3a. Create service account key\n",
    "If we want to use Google Cloud services like Google Natural Language, we need a service account key. This is like our credential to use Google’s services.\n",
    "\n",
    "Go to Google Developers Console, click “Credentials” tab, choose “Create credentials” and click “Service account key”.\n",
    "\n",
    "There is a .json file that will be automatically downloaded, name it creds.json.\n",
    "\n",
    "Set the GOOGLE_APPLICATION_CREDENTIALS with the path of our creds.json file in the terminal.\n",
    "\n",
    "```\n",
    "export GOOGLE_APPLICATION_CREDENTIALS='[PATH_TO_CREDS.JSON]'\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Write the program\n",
    "This program will gather all the tweets containing the defined keyword in the last 24 hours with a maximum of 50 tweets. Then it will analyze the tweets’ sentiments one by one. We will send the result (average sentiment score) through Telegram chat.\n",
    "\n",
    "This is a simple workflow of our program.\n",
    "\n",
    "> connect to the Twitter API -> search tweets based on the keyword -> clean all of the tweets -> get tweet’s sentiment score -> send the result\n",
    "\n",
    "### 4a. Connect to the Twitter API\n",
    "The first thing that we need to do is gather the tweets’ data, so we have to connect to the Twitter API first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the tweepy library.\n",
    "import tweepy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the keys that we generated earlier.\n",
    "\n",
    "# Twitter API Credentials\n",
    "CONS_KEY = \"TOKEN_HERE\"\n",
    "CONS_SECRET = \"TOKEN_HERE\"\n",
    "ACC_TOKEN = \"TOKEN_HERE\"\n",
    "ACC_SECRET = \"TOKEN_HERE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a function called authentication to connect to the API, with four parameters which are all of the keys.\n",
    "\n",
    "def authentication(cons_key, cons_secret, acc_token, acc_secret):\n",
    "    auth = tweepy.OAuthHandler(cons_key, cons_secret)\n",
    "    auth.set_access_token(acc_token, acc_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    return api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Search the tweets\n",
    "We can search the tweets with two criteria, based on time or quantity. If it’s based on time, we define the time interval and if it’s based on quantity, we define the total tweets that we want to gather. Since we want to gather the tweets from the last 24 hours with maximum tweets of 50, we will use both of the criteria.\n",
    "\n",
    "Since we want to gather the tweets from the last 24 hours, let's take yesterday’s date as our time parameter.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "today_datetime = datetime.today().now()\n",
    "yesterday_datetime = today_datetime - timedelta(days=1)\n",
    "today_date = today_datetime.strftime('%Y-%m-%d')\n",
    "yesterday_date = yesterday_datetime.strftime('%Y-%m-%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to the Twitter API using a function we defined before.\n",
    "\n",
    "api = authentication(CONS_KEY,CONS_SECRET,ACC_TOKEN,ACC_SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "HASHTAG = ['#ppmadrid', '#csxmadrid', '#28M', '#eleccionesMadrid','#MásMadrid','#MasMadrid','#VOX','#EspañaViva','#psoe','#ElMadridQueQuieres']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "def rest_tweets(self, query, lang=\"es\", limit=None):\n",
    "        \"\"\"\n",
    "        returns all the tweets within 7 days top according to the query received by this method\n",
    "        returns the complete tweet\n",
    "        :param query: should contain all the words and can include logic operators\n",
    "        should also provide the period of time for the search\n",
    "        ex: rock OR axe \n",
    "        (visit https://dev.twitter.com/rest/public/search to see how to create a query)\n",
    "        :param lang: the language of the tweets\n",
    "        :param limit: defines the maximum amount of tweets to fetch\n",
    "        :return: tweets: a list of all tweets obtained after the request\n",
    "        \"\"\"\n",
    "        tweets = []\n",
    "\n",
    "        for tweet in tw.Cursor(self.api.search, q=query, lang=lang).items(limit):\n",
    "            tweets.append(tweet._json)\n",
    "\n",
    "        return tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_hashtag(hashtag):\n",
    "    for status in tweepy.Cursor(api.search, q=HASHTAG).items(1000):\n",
    "        try:\n",
    "            for media in status.extended_entities['media']:\n",
    "                print(media['media_url'])\n",
    "                urllib.request.urlretrieve(media['media_url'], os.path.join(os.getcwd(), os.path.join('files', 'riko_meme'), media['media_url'].link.split('/')[-1]))\n",
    "        except AttributeError:\n",
    "            pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_hashtag(HASHTAG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyzetweets(self, access_token, access_token_secret, mytweets=False, q=None):\n",
    "    auth = tweepy.OAuthHandler(CONSUMER_KEY, CONSUMER_SECRET)\n",
    "    auth.set_access_token(access_token, access_token_secret)\n",
    "    api = tweepy.API(auth)\n",
    "    sentimentlist = []\n",
    "    subjectivitylist = []\n",
    "    number = NUMBER_OF_TWEETS\n",
    "    tweets = tweepy.Cursor(api.user_timeline).items() if mytweets else tweepy.Cursor(api.search, q=q).items(number)\n",
    "    for index, tweet in enumerate(tweets):\n",
    "        analysis = TextBlob(tweet.text).sentiment\n",
    "        sentimentlist.append(analysis.polarity)\n",
    "        subjectivitylist.append(analysis.subjectivity)\n",
    "        self.update_state(state=\"RUNNING\", meta={\"current\": index + 1, \"total\": number})\n",
    "    sentimentavg = float(sum(sentimentlist) / max(len(sentimentlist), 1))\n",
    "    subjectivityavg = float(sum(subjectivitylist) / max(len(subjectivitylist), 1))\n",
    "    return {\"current\": number, \"total\": number, \"subjectivityavg\": subjectivityavg, \"sentimentavg\": sentimentavg} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define our search parameters. q is where we define our keyword, since is the start date for our search, result_type='recent' means we are going to take the newest tweets, lang='en' is going to take the English tweets only, and items(total_tweets) is where we define the maximum tweets that we are going to take."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'numbers' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c5d9eeea918f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m                               \u001b[0mresult_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'recent'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                               \u001b[0mexclude_replies\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m                               lang='es').items(numbers)\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'numbers' is not defined"
     ]
    }
   ],
   "source": [
    "search_result = tweepy.Cursor(api.search, \n",
    "                              q=HASHTAG, \n",
    "                              since=yesterday_date,\n",
    "                              result_type='recent',\n",
    "                              exclude_replies = True,\n",
    "                              lang='es').items(numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wrap those codes in a function called search_tweets with keyword and total_tweets as the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search_tweets(keyword, total_tweets):\n",
    "    today_datetime = datetime.today().now()\n",
    "    yesterday_datetime = today_datetime - timedelta(days=1)\n",
    "    today_date = today_datetime.strftime('%Y-%m-%d')\n",
    "    yesterday_date = yesterday_datetime.strftime('%Y-%m-%d')\n",
    "    api = authentication(CONS_KEY,CONS_SECRET,ACC_TOKEN,ACC_SECRET)\n",
    "    search_result = tweepy.Cursor(api.search, \n",
    "                                  q=HASHTAG, \n",
    "                                  since=yesterday_date, \n",
    "                                  result_type='recent', \n",
    "                                  lang='es').items(total_tweets)\n",
    "    return search_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
