{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "04_Challenge_Spark",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7hg0H7KlcugT"
      },
      "source": [
        "## Spark ML\n",
        "\n",
        "![Spark Logo](http://spark-mooc.github.io/web-assets/images/ta_Spark-logo-small.png) + ![Python Logo](http://spark-mooc.github.io/web-assets/images/python-logo-master-v3-TM-flattened_small.png)\n",
        "## Challenge n.04\n",
        "### Practica sobre cómo generar un flujo de ejecución en un problema de Machine Learning\n",
        "\n",
        "Esta práctica simula un ejercicio completo de ETL (Extract-Transform-Load) junto a un análisis exploratorio de un dataset real, para posteriormente aplicar differentes algoritmos de aprendizaje automático que resuelvan un problema de regresión.\n",
        "\n",
        "#### Contenido del ejercicio\n",
        "\n",
        "* *Conocimiento del dominio*\n",
        "* *Parte 1: Extracción, transformación y carga [ETL] del dataset* (2 punto sobre 10)\n",
        "* *Parte 2: Explorar los datos* (2 puntos sobre 10)\n",
        "* *Parte 3: Visualizar los datos* (2 puntos sobre 10)\n",
        "* *Parte 4: Preparar los datos* (1 puntos sobre 10)\n",
        "\n",
        "NO REALIZAR (octubre 2021)\n",
        "* *Parte 5: Modelar los datos* (3 puntos sobre 10)\n",
        "\n",
        "*Nuestro objetivo será predecir de la forma más exacta posible la energía generada por un conjunto de plantas eléctricas usando los datos generados por un conjunto de sensores.*\n",
        "\n",
        "\n",
        "## Conocimiento del dominio\n",
        "\n",
        "### Background \n",
        "\n",
        "La generación de energía es un proceso complejo, comprenderlo para poder predecir la potencia de salida es un elemento vital en la gestión de una planta energética y su conexión a la red. Los operadores de una red eléctrica regional crean predicciones de la demanda de energía en base a la información histórica y los factores ambientales (por ejemplo, la temperatura). Luego comparan las predicciones con los recursos disponibles (por ejemplo, plantas, carbón, gas natural, nuclear, solar, eólica, hidráulica, etc). Las tecnologías de generación de energía, como la solar o la eólica, dependen en gran medida de las condiciones ambientales, pero todas las centrales eléctricas son objeto de mantenimientos tanto planificados y como puntuales debidos a un problema.\n",
        "\n",
        "En esta practica usaremos un ejemplo del mundo real sobre la demanda prevista (en dos escalas de tiempo), la demanda real, y los recursos disponibles de la red electrica de California: http://www.caiso.com/Pages/TodaysOutlook.aspx\n",
        "\n",
        "![](http://content.caiso.com/outlook/SP/ems_small.gif)\n",
        "\n",
        "El reto para un operador de red de energía es cómo manejar un déficit de recursos disponibles frente a la demanda real. Hay tres posibles soluciones a un déficit de energía: construir más plantas de energía base (este proceso puede costar muchos anos de planificación y construcción), comprar e importar de otras redes eléctricas regionales energía sobrante (esta opción puede ser muy cara y está limitado por las interconexiones entre las redes de transmisión de energía y el exceso de potencia disponible de otras redes), o activar pequeñas [plantas de pico](https://en.wikipedia.org/wiki/Peaking_power_plant). Debido a que los operadores de red necesitan responder con rapidez a un déficit de energía para evitar un corte del suministro, estos basan sus decisiones en una combinación de las dos últimas opciones. En esta práctica, nos centraremos en la última elección.\n",
        "\n",
        "### La lógica de negocio\n",
        "\n",
        "Debido a que la demanda de energía solo supera a la oferta ocasionalmente, la potencia suministrada por una planta de energía pico tiene un precio mucho más alto por kilovatio hora que la energía generada por las centrales eléctricas base de una red eléctrica. Una planta pico puede operar muchas horas al día, o solo unas pocas horas al año, dependiendo de la condición de la red eléctrica de la región. Debido al alto coste de la construcción de una planta de energía eficiente, si una planta pico solo va a funcionar por un tiempo corto o muy variable, no tiene sentido económico para que sea tan eficiente como una planta de energía base. Además, el equipo y los combustibles utilizados en las plantas base a menudo no son adecuados para uso en plantas de pico.\n",
        "\n",
        "La salida de potencia de una central eléctrica pico varía dependiendo de las condiciones ambientales, por lo que el problema de negocio a resolver se podría describir como _predecir la salida de potencia de una central eléctrica pico en función de la condiciones ambientales_  - ya que esto permitiría al operador de la red hacer compensaciones económicas sobre el número de plantas pico que ha de conectar en cada momento (o si por el contrario le interesa comprar energía más cara de otra red).\n",
        "\n",
        "Una vez descrita esta lógica de negocio, primero debemos proceder a realizar un análisis exploratorio previo y trasladar el problema de negocio (predecir la potencia de salida en función de las condiciones medio ambientales) en un tarea de aprendizaje automático (ML). Por ejemplo, una tarea de ML que podríamos aplicar a este problema es la regresión, ya que tenemos un variable objetivo (dependiente) que es numérica. Para esto usaremos [Apache Spark ML Pipeline](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark-ml-package) para calcular dicha regresión.\n",
        "\n",
        "Los datos del mundo real que usaremos en esta práctica se componen de 9.568 puntos de datos, cada uno con 4 atributos ambientales recogidos en una Central de Ciclo Combinado de más de 6 años (2006-2011), proporcionado por la Universidad de California, Irvine en [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)). Para más detalles sobre el conjunto de datos visitar la página de la UCI, o las siguientes referencias:\n",
        "\n",
        "* Pinar Tufekci, [Prediction of full load electrical power output of a base load operated combined cycle power plant using machine learning methods](http://www.journals.elsevier.com/international-journal-of-electrical-power-and-energy-systems/), International Journal of Electrical Power & Energy Systems, Volume 60, September 2014, Pages 126-140, ISSN 0142-0615.\n",
        "* Heysem Kaya, Pinar Tufekci and Fikret S. Gurgen: [Local and Global Learning Methods for Predicting Power of a Combined Gas & Steam Turbine](http://www.cmpe.boun.edu.tr/~kaya/kaya2012gasturbine.pdf), Proceedings of the International Conference on Emerging Trends in Computer and Electronics Engineering ICETCEE 2012, pp. 13-18 (Mar. 2012, Dubai)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W-KDGCvzdEY4"
      },
      "source": [
        "**Tarea a realizar durante la primera parte:**Revisar la documentacion y referencias de:\n",
        "* [Spark Machine Learning Pipeline](https://spark.apache.org/docs/latest/ml-guide.html#main-concepts-in-pipelines).\n",
        "19:06\n",
        "\n",
        "## Parte 1: Extracción, transformación y carga [ETL] del dataset\n",
        "\n",
        "Ahora que entendemos lo que estamos tratando de hacer, el primer paso consiste en cargar los datos en un formato que podemos consultar y utilizar fácilmente. Esto se conoce como ETL o \"extracción, transformación y carga\". Primero, vamos a cargar nuestro archivo de HDFS.Nuestros datos están disponibles en la siguiente ruta:\n",
        "\n",
        "```\n",
        "/carpeta-datos/pra2\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hd9KuYl7nvnO"
      },
      "source": [
        "### Ejercicio 1(a)\n",
        "\n",
        "Empezaremos por visualizar una muestra de los datos. Para esto usaremos las funciones de hdfs para explorar el contenido del directorio de trabajo:/carpeta/datos/pra2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q0LZrhIinwTi"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGxHzT0zn3S7"
      },
      "source": [
        "==========================================================================================================================================================================================================\n",
        "### Ejercicio 1(b)\n",
        "\n",
        "Ahora usaremos PySpark para visualizar las 5 primeras líneas de los datos\n",
        "\n",
        "*Hint*: Primero crea un RDD a partir de los datos usando [`sc.textFile()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.SparkContext.textFile).\n",
        "\n",
        "*Hint*: Luego piensa como usar el RDD creado para mostrar datos, el método [`take()`](https://spark.apache.org/docs/latest/api/python/pyspark.html#pyspark.RDD.take) puede ser una buena opción a considerar."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L76v_1osn4Jo"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GYmFUlcRn7n8"
      },
      "source": [
        "A partir nuestra exploración inicial de una muestra de los datos, podemos hacer varias observaciones sobre el proceso de ETL:\n",
        "- Los datos son un conjunto de .csv (archivos con valores separados por coma) \n",
        "- Hay una fila de cabecera, que es el nombre de las columnas\n",
        "- Parece que el tipo de los datos en cada columna es constante (es decir, cada columna es de tipo double)\n",
        "\n",
        "El esquema de datos que hemos obtenido de UCI es:\n",
        "- AT = Atmospheric Temperature in C\n",
        "- V = Exhaust Vacuum Speed\n",
        "- AP = Atmospheric Pressure\n",
        "- RH = Relative Humidity\n",
        "- PE = Power Output.  Esta es la variable dependiente que queremos predecir usando los otras cuatro\n",
        "\n",
        "Para usar el paquete Spark CSV, usaremos el método [sqlContext.read.format()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.format) para especificar el formato de la fuente de datos de entrada: `'csv'`\n",
        "\n",
        "Podemos especificar diferentes opciones de como importar los datos usando el método [options()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.options).\n",
        "\n",
        "Usaremos las siguientes opciones:\n",
        "- `delimiter=','` porque nuestros datos se encuentran delimitados por comas\n",
        "- `header='true'` porque nuestro dataset tiene una fila que representa la cabecera de los datos\n",
        "- `inferschema='true'` porque creemos que todos los datos son números reales, por lo tanto la librería puede inferir el tipo de cada columna de forma automática.\n",
        "\n",
        "El ultimo componente necesario para crear un DataFrame es determinar la ubicación de los datos usando el método [load()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrameReader.load).\n",
        "\n",
        "Juntando todo, usaremos la siguiente operación:\n",
        "\n",
        "`sqlContext.read.format().options().load()`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IvnbjfIbn8Nc"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orRJl30zoBNf"
      },
      "source": [
        "### Ejercicio 1(c)\n",
        "\n",
        "Crear un DataFrame a partir de los datos.\n",
        "- El formato es csv\n",
        "\n",
        "En el campo opciones incluiremos 3, formadas por nombre de opción y valor, separadas por coma.\n",
        "- El separador es el tabulador\n",
        "- El fichero contiene cabecera 'header'\n",
        "- Para crear un dataframe necesitamos un esquema (schema). A partir de los datos Spark puede tratar de inferir el esquema, le diremos 'true'.\n",
        "\n",
        "El directorio a cargar es el especificado anteriormente. Es importante indicarle a Spark que es una ubicación ya montada en el sistema dbfs, como se ha mostrado en el ejercicio 2a."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moYNiuNloB--"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWJ2FJfeoFJD"
      },
      "source": [
        "Ahora en lugar de usar [spark csv](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html) para inferir (inferSchema()) los tipos de las columnas, especificaremos el esquema como [DataType](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DataType), el cual es una lista de [StructField](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.StructType).\n",
        "\n",
        "La lista completa de tipos se encuentra en el modulo [pyspark.sql.types](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#module-pyspark.sql.types). Para nuestros datos, usaremos [DoubleType()](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.types.DoubleType).\n",
        "\n",
        "Por ejemplo, para especificar cual es el nombre de la columna usaremos: `StructField(`_name_`,` _type_`, True)`. (El tercer parámetro, `True`, significa que permitimos que la columna tenga valores null.)\n",
        "\n",
        "### Ejercicio 1(d)\n",
        "\n",
        "Crea un esquema a medida para el dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guWdLunEoFtE"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aWUa4vazoIBB"
      },
      "source": [
        "### Exercicio 1(e)\n",
        "\n",
        "Ahora, usaremos el esquema que acabamos de crear para leer los datos. Para realizar esta operación, modificaremos el paso anterior `sqlContext.read.format`. Podemos especificar el esquema haciendo:\n",
        "- Anadir `schema = customSchema` al método load (simplemente anadelo usando una coma justo después del nombre del archivo)\n",
        "- Eliminado la opción `inferschema='true'` ya que ahora especificamos el esquema que han de seguir los datos"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CQVDh3EJoImP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJeAm2zydNmZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FJIJd1GXdNep"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fv1vuOHadNUw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIDe1nq2dHLa"
      },
      "source": [
        "## Parte 2: Explorar tus Datos\n",
        "\n",
        "### Ejercicio 2(a)\n",
        "\n",
        "​\n",
        "\n",
        "Ahora que ya hemos cargado los datos, el siguiente paso es explorarlos y realizar algunos análisis y visualizaciones básicas.\n",
        "\n",
        "​\n",
        "\n",
        "Este es un paso que siempre se debe realizar **antes de** intentar ajustar un modelo a los datos, ya que este paso muchas veces nos permitirá conocer una gran información sobre los datos.\n",
        "\n",
        "En primer lugar vamos a registrar nuestro DataFrame como una tabla de SQL llamado power_plant. Debido a que es posible que repitas esta práctica varias veces, vamos a tomar la precaución de eliminar cualquier tabla existente en primer lugar.\n",
        "\n",
        "Una vez ejecutado el paso anterior, podemos registrar nuestro DataFrame como una tabla de SQL usando sqlContext.registerDataFrameAsTable().\n",
        "\n",
        "Crea una tabla llamada power_plant con las indicaciones mostradas.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V-uKshkroMSX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mAIU8YSloLqp"
      },
      "source": [
        "Ahora que nuestro DataFrame existe como una tabla SQL, podemos explorarlo utilizando comandos SQL y `sqlContext.sql(...)`. Utiliza la función `show()` para visualizar el resultado del dataframe."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ZwUkXLroOvy"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J7HMxo3KoRi6"
      },
      "source": [
        "**Definición de Esquema**\n",
        "\n",
        "Una vez más, nuestro esquema es el siguiente:\n",
        "\n",
        "- AT = Atmospheric Temperature in C\n",
        "- V = Exhaust Vacuum Speed\n",
        "- AP = Atmospheric Pressure\n",
        "- RH = Relative Humidity\n",
        "- PE = Power Output\n",
        "\n",
        "PE es nuestra variable objetivo. Este es el valor que intentamos predecir usando las otras mediciones.\n",
        "\n",
        "*Referencia [UCI Machine Learning Repository Combined Cycle Power Plant Data Set](https://archive.ics.uci.edu/ml/datasets/Combined+Cycle+Power+Plant)*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HMOlu6pZoSNZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0e2gpHdoVh5"
      },
      "source": [
        "## 2b\n",
        "\n",
        "Ahora vamos a realizar un análisis estadístico básico de todas las columnas.\n",
        "\n",
        "Calculad y mostrad los resultados en modo tabla (la función `show()` os puede ser de ayuda):\n",
        "* Número de registros en nuestros datos\n",
        "* Media de cada columna\n",
        "* Máximo y mínimo de cada columna\n",
        "* Desviación estándar de cada columna\n",
        "\n",
        "Hint: Revisad [DataFrame](https://spark.apache.org/docs/latest/api/python/pyspark.sql.html#pyspark.sql.DataFrame) ya que contiene métodos que permiten realizar dichos cálculos de manera sencilla."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SL-qrDURoWQZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n-531IPXdO2O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "knrDFDT-dStz"
      },
      "source": [
        "## Parte 3: Visualizar los datos\n",
        "\n",
        "Para entender nuestros datos, intentamos buscar correlaciones entre las diferentes características y sus correspondientes etiquetas. Esto puede ser importante cuando seleccionamos un modelo. Por ejemplo, si una etiqueta y sus características se correlacionan de forma lineal, un modelo de regresión lineal obtendrá un buen rendimiento; por el contrario si la relación es no lineal, modelos más complejos, como arboles de decisión pueden ser una mejor opción. Podemos utilizar herramientas de visualización para observar cada uno de los posibles predictores en relación con la etiqueta como un gráfico de dispersión para ver la correlación entre ellos.\n",
        "\n",
        "============================================================================\n",
        "### Ejercicio 3(a)\n",
        "\n",
        "#### Añade las siguientes figuras: \n",
        "Vamos a ver si hay una correlación entre la temperatura y la potencia de salida. Podemos utilizar una consulta SQL para crear una nueva tabla que contenga solo el de temperatura (AT) y potencia (PE), y luego usar un gráfico de dispersión con la temperatura en el eje X y la potencia en el eje Y para visualizar la relación (si la hay) entre la temperatura y la energía.\n",
        "\n",
        "Realiza los siguientes pasos:\n",
        "- Carga una muestra de datos aleatorios de 1000 pares de valores para PE y AT. Puedes utilizar una ordenación aleatoria o un sample() sobre el resultado. Para hacer el plot puedes hacer un collect().\n",
        "- Utiliza matplotlib y Pandas para hacer un scatter plot (https://matplotlib.org/3.1.1/api/_as_gen/matplotlib.pyplot.scatter.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HjPymHPwdTNS"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bvf0HNGtoda6"
      },
      "source": [
        "### Ejercicio 3(b)\n",
        "\n",
        "Repitiendo el proceso anterior, usa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Exhaust Vacuum Speed (V)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bIt7yBCoeEY"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w-_9lwvNoins"
      },
      "source": [
        "Ahora vamos a repetir este ejercicio con el resto de variables y la etiqueta Power Output.\n",
        "\n",
        "### Ejercicio 3(c)\n",
        "\n",
        "Usa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Pressure (AP)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfL062ExojIH"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0mYCB5doloY"
      },
      "source": [
        "### Ejercicio 3(d)\n",
        "\n",
        "Usa una sentencia SQL para crear un gráfico de dispersión entre las variables Power (PE) y Humidity (RH)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwccVDtgomJ5"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EHWlIm24olRu"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GM1CIFCdWln"
      },
      "source": [
        "## Parte 4: Preparación de los datos\n",
        "\n",
        "El siguiente paso es preparar los datos para aplicar la regresión. Dado que todo el dataset es numérico y consistente, esta será una tarea sencilla y directa.\n",
        "\n",
        "El objetivo es utilizar el método de regresión para determinar una función que nos de la potencia de salida como una función de un conjunto de características de predicción. El primer paso en la construcción de nuestra regresión es convertir las características de predicción de nuestro DataFrame a un vector de características utilizando el método [pyspark.ml.feature.VectorAssembler()](https://spark.apache.org/docs/latest/api/python/pyspark.ml.html#pyspark.ml.feature.VectorAssembler).\n",
        "\n",
        "El VectorAssembler es una transformación que combina una lista dada de columnas en un único vector. Esta transformación es muy útil cuando queremos combinar características en crudo de los datos con otras generadas al aplicar diferentes funciones sobre los datos en un único vector de características. Para integrar en un único vector toda esta información antes de ejecutar un algoritmo de aprendizaje automático, el VectorAssembler toma una lista con los nombres de las columnas de entrada (lista de strings) y el nombre de la columna de salida (string)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VDg7tFzxdXd8"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxDTmFJOdFkg"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XsQ3RfIqcMBQ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}